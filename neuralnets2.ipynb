{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "85b65288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+math.exp(-x))\n",
    "\n",
    "def sigmoidderivative(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "def OneHot(int):\n",
    "    output = np.array([0,0,0,0,0,0,0,0,0,0])\n",
    "    output[int] = 1\n",
    "    return output\n",
    "\n",
    "vec_sigmoid = np.vectorize(sigmoid)\n",
    "vec_sigmoidderivative = np.vectorize(sigmoidderivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4d98633",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLayer:\n",
    "\n",
    "    def __init__(self, numinputs:int, numoutputs:int, activation=None):\n",
    "\n",
    "        self.numinputs = numinputs\n",
    "        self.numoutputs = numoutputs\n",
    "        self.activation = activation\n",
    "        self.weights = np.random.randn(self.numoutputs, self.numinputs + 1)\n",
    "\n",
    "    \n",
    "    def Evaluate(self, inputs):\n",
    "\n",
    "        inputs = np.append(inputs, np.array([1]))\n",
    "    \n",
    "        outputs = self.weights @ inputs # this is \\vec{h}\n",
    "\n",
    "        match self.activation:\n",
    "            case \"Sigmoid\":\n",
    "                outputs = vec_sigmoid(outputs)\n",
    "\n",
    "            case \"Softmax\":\n",
    "                denom = 0\n",
    "                for i in range(len(outputs)):\n",
    "                    denom += math.exp[outputs[i]]\n",
    "                    outputs[i] = math.exp(outputs[i])\n",
    "                outputs = outputs/denom\n",
    "\n",
    "            case \"ReLU\":\n",
    "                # Put code here, delete the pass\n",
    "                for i in range(len(outputs)):\n",
    "                    if outputs[i] > 0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        outputs[i] = 0\n",
    "                pass #Alejandro shall not pass\n",
    "            \n",
    "            case \"Tanh\":\n",
    "                # Put code here, delete the pass\n",
    "                outputs = (np.exp(outputs) - np.exp((-1*outputs))) / (np.exp(outputs) + np.exp((-1*outputs)))\n",
    "\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    \n",
    "        \n",
    "    def ComputeLocalGradient(self, inputs):\n",
    "        # z is output after activation\n",
    "        # h is output after linear layer\n",
    "        # w are weights\n",
    "        # Need to compute three things:\n",
    "        # dz/dh\n",
    "        # dh/dw\n",
    "        # dh/dx\n",
    "\n",
    "        inputs = np.append(inputs, np.array([1]))\n",
    "        outputs = self.weights @ inputs\n",
    "\n",
    "\n",
    "        # This part computes dzdh, and has cases for various activation functions\n",
    "        match self.activation:\n",
    "            case \"Sigmoid\":\n",
    "                dzdh = np.diag(vec_sigmoidderivative(outputs))\n",
    "            case \"Softmax\":\n",
    "                n = len(outputs)\n",
    "                dzdh = np.zeros((n, n))\n",
    "                denom = 0\n",
    "                for i in range(n):\n",
    "                    denom += math.exp(outputs[i])\n",
    "                \n",
    "                for i in range(n):\n",
    "                    for j in range(n):\n",
    "                        if i == j:\n",
    "                            dzdh[i][j] = (denom * math.exp(outputs[i]) - (math.exp(outputs[i])**2))/(denom**2)\n",
    "                        else:\n",
    "                            dzdh[i][j] = -(math.exp(outputs[j]))*(math.exp(outputs[j]))/(denom**2)\n",
    "\n",
    "            case \"ReLU\":\n",
    "                # Put code here and remove pass\n",
    "                n = len(outputs)\n",
    "                dzdh = np.zeros((n,n))\n",
    "                for i in range(n):\n",
    "                    if outputs[i] > 0:\n",
    "                        dzdh[i,i] = 1\n",
    "                    else:\n",
    "                        dzdh[i,i] = 0\n",
    "                    \n",
    "\n",
    "            case \"Tanh\":\n",
    "                # Put code here and remove pass\n",
    "                dzdh = np.diag(1 - ((np.exp(outputs) - np.exp((-1*outputs))) / (np.exp(outputs) + np.exp((-1*outputs))))**2)\n",
    "            \n",
    "\n",
    "        \n",
    "        # This part computes dhdw        \n",
    "        dhdw = np.zeros((self.numoutputs, self.numoutputs, self.numinputs+1)) #because of bias\n",
    "        for i in range(self.numoutputs):\n",
    "            for j in range(self.numinputs):\n",
    "                dhdw[i,i,j] = inputs[i]\n",
    "            dhdw[i,i,self.numinputs] = 1\n",
    "            \n",
    "        # This part computes dhdx\n",
    "        dhdx = self.weights[:, :-1]\n",
    "\n",
    "\n",
    "        return (dzdh, dhdw, dhdx)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35480923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "(3, 3, 6)\n",
      "(3, 5)\n"
     ]
    }
   ],
   "source": [
    "Layer1 = NeuralLayer(5,3,\"Tanh\")\n",
    "#Layer1.weights = np.array([[1, 2, -1], [3, -2, 1]])\n",
    "test1 = np.array([1, 2,3,4,5])\n",
    "(dzdh, dhdw, dhdx) = Layer1.ComputeLocalGradient(test1)\n",
    "print(dzdh.shape)\n",
    "print(dhdw.shape)\n",
    "print(dhdx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79a744c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, errorfunc=None):\n",
    "        \n",
    "        self.errorfunc = errorfunc\n",
    "        self.layers = []\n",
    "        self.numlayers = 0\n",
    "\n",
    "    def AppendLayer(self, layer: NeuralLayer):\n",
    "        # need to check that the new layer to be appended has same \n",
    "        # number of inputs as the last layer already in the network\n",
    "        if len(self.layers) > 0:\n",
    "            if layer.numinputs == self.layers[-1].numoutputs:\n",
    "                self.layers.append(layer)\n",
    "                self.numlayers += 1\n",
    "            else:\n",
    "                print(\"Error: number of inputs does not match previous layer\")\n",
    "        else:\n",
    "            self.layers.append(layer)\n",
    "            self.numlayers += 1\n",
    "\n",
    "        \n",
    "    def Evaluate(self, inputs):\n",
    "\n",
    "        outputs = []\n",
    "        outputs.append(self.layers[0].Evaluate(inputs))\n",
    "\n",
    "        for i in range(1,self.numlayers):\n",
    "            outputs.append(self.layers[i].Evaluate(outputs[i-1]))\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def ComputeError(self, inputs, trueoutputs):\n",
    "\n",
    "        outputs = self.Evaluate(inputs)\n",
    "        \n",
    "        if self.errorfunc == \"MSE\":\n",
    "            n = len(outputs[-1])\n",
    "            diffs = outputs[-1] - trueoutputs\n",
    "            err = np.dot(diffs, diffs)\n",
    "            err = err/(2*n)\n",
    "            return err\n",
    "\n",
    "    def BackPropagate(self, inputs, trueoutputs, learningrate):\n",
    "\n",
    "        outputs = self.Evaluate(inputs)\n",
    "        gradients = []\n",
    "\n",
    "        # Compute all the necessary gradients\n",
    "        for i in range(self.numlayers):\n",
    "            if i == 0:\n",
    "                tempinput = inputs\n",
    "            else:\n",
    "                tempinput = outputs[i-1]\n",
    "            \n",
    "            gradients.append(self.layers[i].ComputeLocalGradient(tempinput))\n",
    "\n",
    "        match self.errorfunc:\n",
    "            case \"MSE\":\n",
    "                dldz = (0.5) * (outputs[-1] - trueoutputs)\n",
    "\n",
    "            case \"CrossEntropy\":\n",
    "                dldz = np.zeros(len(trueoutputs))\n",
    "                spot = np.where(1 == trueoutputs)\n",
    "                dldz[spot] = 1/outputs[-1][spot]\n",
    "                \n",
    "            \n",
    "\n",
    "        # Update weights, working backwards\n",
    "\n",
    "        currgrad = dldz @ gradients[-1][0]\n",
    "    \n",
    "        for i in range(self.numlayers-1, -1, -1):\n",
    "            self.layers[i].weights -= learningrate * (currgrad @ gradients[i][1])\n",
    "            currgrad = currgrad @ gradients[i][0] @ gradients[i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef2ccb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11836143679049575\n",
      "244.78737838563907\n",
      "0.25\n",
      "0.25\n",
      "0.25\n",
      "0.25\n",
      "0.25\n",
      "0.25\n",
      "0.25\n",
      "0.25\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "MyNN = NeuralNetwork(errorfunc=\"MSE\")\n",
    "MyLayer1 = NeuralLayer(5, 3, \"Sigmoid\")\n",
    "MyLayer2 = NeuralLayer(3, 2, \"Sigmoid\")\n",
    "\n",
    "\n",
    "MyNN.AppendLayer(MyLayer1)\n",
    "MyNN.AppendLayer(MyLayer2)\n",
    "\n",
    "myinput = np.array([1,2,3,4,5])\n",
    "mytrue = np.array([1,0])\n",
    "\n",
    "print(MyNN.ComputeError(myinput, mytrue))\n",
    "\n",
    "for i in range(10):\n",
    "    MyNN.BackPropagate(myinput, mytrue, 1)\n",
    "    print(MyNN.ComputeError(myinput, mytrue))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b2becb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from MNIST database\n",
    "(x_train0, y_train0), (x_test0, y_test0) = tf.keras.datasets.mnist.load_data()\n",
    "assert x_train0.shape == (60000, 28, 28)\n",
    "assert x_test0.shape == (10000, 28, 28)\n",
    "assert y_train0.shape == (60000,)\n",
    "assert y_test0.shape == (10000,)\n",
    "\n",
    "# Prepare data for processing\n",
    "# x_train and x_test need to be reshaped and converted to np.float64\n",
    "# y_train and y_test need to be one-hot encoded\n",
    "x_train = [x.flatten().astype(np.float64) / 255.0 for x in x_train0[0:6000]]\n",
    "x_test = [x.flatten().astype(np.float64) / 255.0 for x in x_test0[0:1000]]\n",
    "y_train = [OneHot(y) for y in y_train0[0:6000]]\n",
    "y_test = [OneHot(y) for y in y_test0[0:6000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f0adc12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4488643126069637\n",
      "0.44883925223147336\n",
      "0.44881307138077925\n",
      "0.44878569414590974\n",
      "0.44875703767224684\n",
      "0.44872701135339155\n",
      "0.44869551591095025\n",
      "0.44866244234108865\n",
      "0.4486276707049529\n",
      "0.4485910687354836\n",
      "0.4485524902275152\n",
      "0.44851177317110524\n",
      "0.44846873757940575\n",
      "0.44842318295163786\n",
      "0.4483748852982366\n",
      "0.4483235936382311\n",
      "0.4482690258573646\n",
      "0.4482108637879577\n",
      "0.4481487473362085\n",
      "0.44808226743699997\n",
      "0.4480109575569039\n",
      "0.4479342833882253\n",
      "0.4478516302740749\n",
      "0.44776228776745197\n",
      "0.4476654305432266\n",
      "0.44756009463229934\n",
      "0.44744514760549314\n",
      "0.44731925086210794\n",
      "0.44718081151735756\n",
      "0.4470279204486675\n",
      "0.4468582717238636\n",
      "0.446669056696608\n",
      "0.44645682320840585\n",
      "0.4462172860973891\n",
      "0.4459450688073036\n",
      "0.44563334606079996\n",
      "0.4452733422501338\n",
      "0.4448536159914019\n",
      "0.4443590224903528\n",
      "0.44376918250888897\n",
      "0.4430561843996087\n",
      "0.4421810804639267\n",
      "0.44108848184719873\n",
      "0.43969819976417507\n",
      "0.4378925672990562\n",
      "0.43549860828750564\n",
      "0.4322688818371926\n",
      "0.4278848017110756\n",
      "0.42206880850213047\n",
      "0.4149972034715681\n",
      "0.40798841199345404\n",
      "0.4031197943151466\n",
      "0.40093515736007657\n",
      "0.40024083850160486\n",
      "0.4000539112071813\n",
      "0.40000624467560525\n",
      "0.399994266973183\n",
      "0.39999126783699296\n",
      "0.39999051689302517\n",
      "0.39999032821921976\n",
      "0.3999902801265845\n",
      "0.3999902671798343\n",
      "0.39999026301933316\n",
      "0.39999026105512075\n",
      "0.39999025963970375\n",
      "0.39999025836120855\n",
      "0.3999902571166666\n",
      "0.3999902558803356\n",
      "0.39999025464577986\n",
      "0.3999902534113903\n",
      "0.39999025217676454\n",
      "0.39999025094180196\n",
      "0.3999902497064771\n",
      "0.39999024847078357\n",
      "0.3999902472347196\n",
      "0.3999902459982847\n",
      "0.3999902447614786\n",
      "0.39999024352430096\n",
      "0.39999024228675173\n",
      "0.3999902410488307\n",
      "0.39999023981053766\n",
      "0.39999023857187255\n",
      "0.39999023733283506\n",
      "0.39999023609342516\n",
      "0.3999902348536425\n",
      "0.3999902336134871\n",
      "0.3999902323729586\n",
      "0.39999023113205706\n",
      "0.39999022989078215\n",
      "0.39999022864913364\n",
      "0.3999902274071115\n",
      "0.3999902261647154\n",
      "0.39999022492194536\n",
      "0.39999022367880105\n",
      "0.39999022243528237\n",
      "0.39999022119138916\n",
      "0.39999021994712114\n",
      "0.3999902187024783\n",
      "0.3999902174574604\n",
      "0.39999021621206715\n",
      "0.3999902149662985\n",
      "0.3999902149662985\n",
      "Final check evaluation: [array([ 9.99922377e-01,  4.44089210e-16, -1.00000000e+00,  9.99988977e-01,\n",
      "       -1.00000000e+00,  9.99999909e-01, -1.00000000e+00,  9.99990793e-01,\n",
      "       -1.00000000e+00, -1.00000000e+00])]\n"
     ]
    }
   ],
   "source": [
    "MyMNISTNetwork = NeuralNetwork(\"MSE\")\n",
    "MyMNISTNetwork.AppendLayer(NeuralLayer(28*28,10,\"Tanh\"))\n",
    "\n",
    "\n",
    "y_train0[0]\n",
    "\n",
    "testinput = np.astype(x_train0[0].reshape(28*28), np.float64)\n",
    "testinput /= 255.0\n",
    "#print(testinput.sum())\n",
    "#print(MyMNISTNetwork.Evaluate(testinput))\n",
    "#print(MyMNISTNetwork.layers[-1].weights.dtype)\n",
    "\n",
    "onehot = np.array([0,0,0,0,0,1,0,0,0,0])\n",
    "\n",
    "print(MyMNISTNetwork.ComputeError(testinput, onehot))\n",
    "for i in range(100):\n",
    "    MyMNISTNetwork.BackPropagate(testinput, onehot, 1)\n",
    "    print(MyMNISTNetwork.ComputeError(testinput, onehot))\n",
    "    \n",
    "print(MyMNISTNetwork.ComputeError(testinput, onehot))\n",
    "\n",
    "print(\"Final check evaluation: \" + str(MyMNISTNetwork.Evaluate(testinput)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnvenv (3.11.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
